{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emnaNaitIbourk/PRODGY_GA_01/blob/main/GA_task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HIrRD-EALsg",
        "outputId": "ca395f0f-1954-42cf-8657-4aa161008cc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers -q\n",
        "!pip install torch -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zw0kJwwhAPqv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "df1b1381-55d4-4b7a-df58-6168424e15f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataset\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:28, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finetuning is complete.Saving model...\n",
            "Model and Tokenizer saved  to  './finetuned_gpt2' .\n"
          ]
        }
      ],
      "source": [
        "#Imports\n",
        "from transformers import GPT2LMHeadModel,GPT2Tokenizer,Trainer,TrainingArguments,TextDataset,DataCollatorForLanguageModeling\n",
        "import torch\n",
        "#Loading the tokenizer and model\n",
        "print(\"Loading tokenizer and model\")\n",
        "tokenizer=GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model=GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "#making sure that the model works  correctly\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    #preparing dataset\n",
        "print(\"Preparing dataset\")\n",
        "dataset=TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"mydataset.txt\",\n",
        "    block_size=128,\n",
        "\n",
        "\n",
        ")\n",
        "#Organizing data into batches for efficient training\n",
        "dataCollator=DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "\n",
        " )\n",
        "#setting training rules\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "training_args=TrainingArguments(\n",
        "    output_dir=\"./finetuned_gpt2\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "\n",
        ")\n",
        "#Finetuning\n",
        "print(\"Starting fine-tuning...\")\n",
        "trainer=Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=dataCollator,\n",
        "    train_dataset=dataset,\n",
        "\n",
        "\n",
        ")\n",
        "#Starting the training process\n",
        "trainer.train()\n",
        "#Saving the finetuned model and its tokenizer to the new folder\n",
        "print(\"Finetuning is complete.Saving model...\")\n",
        "model.save_pretrained(\"./finetuned_gpt2\")\n",
        "tokenizer.save_pretrained(\"./finetuned_gpt2\")\n",
        "print(\"Model and Tokenizer saved  to  './finetuned_gpt2' .\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "my5DdQ-QIyOr"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit -q\n",
        "!pip install pyngrok -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fmIs2cidJSyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cca7e54-0e45-43fb-988b-fa6b6280d0d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        " #Imports for Streamlit and Hugging Face.\n",
        "import streamlit as st\n",
        "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
        "import os\n",
        "\n",
        "# This sets the title for your app's web page.\n",
        "st.title(\"My Custom Text Generator\")\n",
        "\n",
        "# This is the folder where your fine-tuned model is saved.\n",
        "model_path = \"./finetuned_gpt2\"\n",
        "\n",
        "# This part checks if the model exists before trying to use it.\n",
        "if not os.path.exists(model_path):\n",
        "    st.error(f\"Model not found at {model_path}. Please run the training script first.\")\n",
        "else:\n",
        "    # `@st.cache_resource` tells Streamlit to load the model only ONCE, which makes the app fast.\n",
        "    @st.cache_resource\n",
        "    def get_generator():\n",
        "        # `pipeline` is a simple tool that loads the model and tokenizer together for easy text generation.\n",
        "        return pipeline('text-generation', model=model_path, tokenizer=model_path)\n",
        "\n",
        "    generator = get_generator()\n",
        "\n",
        "    # `st.text_area` creates a box where a user can type in their starting sentence.\n",
        "    prompt = st.text_area(\"Enter your prompt:\", \"Once upon a time...\")\n",
        "\n",
        "    # These create sliders on the side of the page to adjust how the text is generated.\n",
        "    max_length = st.sidebar.slider(\"Max text length\", 50, 500, 150)\n",
        "    temperature = st.sidebar.slider(\"Creativity (higher = more random)\", 0.1, 1.0, 0.7)\n",
        "\n",
        "    # `st.button` creates a button that, when clicked, runs the code inside the `if` statement.\n",
        "    if st.button(\"Generate Text\"):\n",
        "        if prompt:\n",
        "            st.write(\"Generating...\")\n",
        "            # `generator(...)` is where the magic happens! It uses your fine-tuned model to continue the text.\n",
        "            generated_text = generator(\n",
        "                prompt,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "            # These lines display the generated text in a clean format on the web page.\n",
        "            st.subheader(\"Generated Text:\")\n",
        "            st.markdown(generated_text[0]['generated_text'])\n",
        "        else:\n",
        "            st.warning(\"Please enter a prompt.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "I36qxr2DLBJU"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/dev/null &\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JvHRqpRhLKxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359f2b0c-565b-45be-a621-039beef4ddf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your public URL: NgrokTunnel: \"https://5a55636f9307.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok authentication token\n",
        "ngrok.set_auth_token(\"31KEfRtpjNt5NBwI442Fu4w57AO_5bViZBm4c6DXSvGTiud3u\")\n",
        "\n",
        "# Open a tunnel to port 8501 (Streamlit default)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Your public URL:\", public_url)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPqI6Hpqc7wKe9kxJWlYuMX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}